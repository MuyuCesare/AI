#### Thinking1：什么是监督学习，无监督学习，半监督学习

监督学习：根据已有的数据集，训练最优模型得到特征与标签之间的关系。数据集的所有数据都标注了标签。监督学习方法包括分类与回归。

无监督学习：根据已有的数据集，训练最优模型得到特征与特征之间的关系(比如聚类的相似性，PCA降维的主成分)。数据集的所有数据都没有标签，只有特征。无监督学习方法主要包括聚类(聚类包括层次聚类和K-Means，凝聚聚类Agglomerative Clustering属于层次聚类)和降维。

​	K-Means：当K确定时，不管选择哪几个点开始聚类，只要迭代次数足够多，结果是一样的。

​		Step1：选取K个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的

​		Step2：将每个点分配到距离最近的类中心点，这样就形成了K个类，然后重新计算每个类的中心点

​		重复Step2，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束

​		两点之间的距离：一般使用欧式距离

​			欧氏距离：直角坐标系中的两点距离公式

​				曼哈顿距离

​				切比雪夫距离

​				余弦距离

​	凝聚聚类Agglomerative Clustering：一种采用自底向上聚类策略的层次聚类算法

​		Step1：将数据集中的每个样本看作一个初始聚类簇

​		Step2：找出距离最近的两个聚类簇进行合并

​		不断重复上述过程，直到达到我们想要的聚类个数

​		如何计算聚类簇之间的距离：![image-20200913100234701](/Users/muyu/Library/Application Support/typora-user-images/image-20200913100234701.png)

​		链接方法：

​			单链接(single-linkage)：最小距离由两个簇的最近样本决定

​			全链接(complete-linkage)：最大距离由两个簇的最远样本决定

​			均链接(average-linkage)：平均距离由两个簇的所有样本共同决定

​			Ward-linkage 算法：通过离差平方和 ESS(Error Sum of Squares)来进行聚类，最小化聚类前后的离方平方和之差

半监督学习：半监督学习介于监督学习和无监督学习之间。数据集中只有少量的数据有标签，同时还会利用没有标签的数据来学习整个数据的潜在分布，提升模型训练的效果。

​	标签传播算法Label Propagation Algorithm：基于标签传播的非重叠社区发现算法

​		现实中存在着各种网络：社交网络，交通网络，交易网络，食物链。将这些行为转化为图的网络形式。

​		原理：基于数据分布上的模型假设，利用少量的已标注数据进行指导并预测未标记数据的标记，并合并到标记数据集中去。

​		步骤：

​			Step1：每个节点拥有独立的标签

​			Step2：标签传播，节点向邻居节点传播自己的标签

​			Step3：标签更新，每个节点的标签更新为邻居节点中出现次数最多的标签。如果存在多个选择，则随机选择一个

​			Step4：如果节点更新后的标签发生了变化，则返回到Step2(激活状态)，否则节点进入非激活状态，如果所有图中所有节点均为非激活状态，则标签更新结束。此时具有相同标签的节点属于同一个社区

#### Thinking2：K-means中的k值如何选取

​	K-means选择k的时候需要将不同k取值的loss(簇间方差累加和)画出来，选择一个权衡点(肘部法，选择靠原点比较近的区域的点)

#### Thinking3：随机森林采用了bagging集成学习，bagging指的是什么

​	bagging思想，将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器

​	bagging不用单棵决策树来做预测，增加了预测准确率，即不容易过拟合，减少了预测方差

​	步骤：样本随机+特征随机

​		如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（采样方式称为bootstrap sample），作为该树的训练集

​		每棵树的训练集都是不同的，而且里面包含重复的训练样本

​			如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的。m越大，树的相关性和分类能力会相应提升。如何选择最优的m是关键问题

​		每棵树都尽最大程度的生长，没有剪枝过程。

​	随机森林分类效果（错误率）：

​		森林中任意两棵树的相关性：相关性越大，错误率越大

​		森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低

#### Thinking4：表征学习和半监督学习的区别是什么

​	表征学习：通过算法让机器自己可以学习到数据的特征，相当于让机器自己做特征工程，而不是人工做。目的是对复杂的原始数据化繁为简，把原始数据的无效信息剔除，把有效信息更有效地进行提炼，形成特征(类似于embedding)，从而使后续的学习任务更简单和精确。深度学习、无监督学习、树模型都属于表征学习。

​	半监督学习：对于只有部分已标记label的数据集，利用没有标签的数据来学习整个数据的潜在分布，提高模型训练效果的方法。标签传播算法属于半监督学习。

​	区别：表征学习的作用是让计算机通过算法提取特征，可以用于监督学习、半监督学习、无监督学习；半监督学习是利用数据集中部分无标签数据来提升有标签数据的模型训练效果。