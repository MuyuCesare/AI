{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯è§†åŒ–æ³¢åŠ¨ç‡ç‰¹å¾\n",
    "from random import gauss\n",
    "from datetime import datetime\n",
    "import tushare as ts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.dates as md\n",
    "\n",
    "# è·å–æ²ªæ·±300çš„æ”¶ç›˜ä»·æ•°æ®å¹¶è®¡ç®—æ”¶ç›Šç‡\n",
    "hs300 = ts.get_k_data('hs300', '2000-01-01').set_index('date').close\n",
    "hs300.index = pd.to_datetime(hs300.index)\n",
    "hs300_return = hs300.pct_change().dropna()   # è®¡ç®—æ—¥æ”¶ç›Šç‡\n",
    "hs300_return.name = 'hs300_return'\n",
    "\n",
    "# å®šä¹‰ä½œå›¾è¾…åŠ©å‡½æ•°\n",
    "def date2num(date: str):\n",
    "    \"\"\"\n",
    "    å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬åŒ–ä¸ºmatplotlibå¯ç”¨çš„æ•°å€¼\n",
    "    \"\"\"\n",
    "    return md.date2num(datetime.strptime(date,'%Y-%m-%d'))\n",
    "\n",
    "def add_marks(time_periods, ax):\n",
    "    \"\"\"\n",
    "    åœ¨axå­å›¾ä¸Šä¸ºtime_periodä¼ å…¥çš„æ—¶é—´èŒƒå›´æ·»åŠ èƒŒæ™¯é¢œè‰²\n",
    "    param time_periods: éœ€è¦æ·»åŠ æ ‡è®°çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œåˆ—è¡¨å…ƒç´ ä¸º(start, end)ç»„æˆçš„å…ƒç»„\n",
    "    param ax: éœ€è¦æ·»åŠ æ ‡è®°çš„å­å›¾\n",
    "    \"\"\"\n",
    "    bottom, top = ax.get_ylim()\n",
    "    height = top - bottom\n",
    "    for start_date, end_date in time_periods:\n",
    "        start = date2num(start_date)\n",
    "        end = date2num(end_date)\n",
    "        width = end - start\n",
    "        rect = Rectangle((start, bottom), width, height, color='y', fill=True, alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# å¯¹ä»·æ ¼å’Œæ—¥æ”¶ç›Šç‡ä½œå›¾\n",
    "f, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw = {'height_ratios':[5,2.5]}, sharex=True)\n",
    "f.set_size_inches(14, 8)\n",
    "\n",
    "ax0.plot(hs300)\n",
    "ax0.set_title('Price')\n",
    "ax1.plot(hs300_return, linewidth=0.5)\n",
    "ax1.set_title('Daily_return')\n",
    "\n",
    "periods = [('2007-10-16', '2008-10-28'), ('2015-06-12', '2016-01-27'), ('2018-01-29', '2018-10-31')]\n",
    "\n",
    "# åœ¨å›¾ä¸Šæ·»åŠ é«˜äº®åŒºåŸŸ\n",
    "add_marks(periods, ax0)\n",
    "add_marks(periods, ax1)\n",
    "plt.show()\n",
    "\n",
    "#æ¨¡æ‹ŸGARCHè¿‡ç¨‹\n",
    "# å‚æ•°è®¾ç½®\n",
    "a0 = 0.06\n",
    "a1 = 0.1\n",
    "b1 = 0.8\n",
    "sigma0 = np.sqrt(a0/(1-a1-b1))\n",
    "sigma0\n",
    "\n",
    "# å®šä¹‰å‡½æ•°æ¨¡æ‹ŸGARCHè¿‡ç¨‹\n",
    "def GARCH(a0, a1, b1, sigma0, T):\n",
    "    \"\"\"\n",
    "    æ¨¡æ‹Ÿä¸€ä¸ªTæ­¥çš„GARCHè¿‡ç¨‹\n",
    "    params a0, a1, b1: å¯¹åº”GARCH(1,1)è¿‡ç¨‹ä¸­çš„ï¼ˆomega, alpha, beta)\n",
    "    params sigma0: åˆå§‹æ³¢åŠ¨ç‡\n",
    "    params T: æ¨¡æ‹Ÿçš„æ€»æ­¥æ•°\n",
    "    return: è¿”å›æ¨¡æ‹Ÿçš„æ³¢åŠ¨ç‡è·¯å¾„\n",
    "    \"\"\"\n",
    "    sigma = [sigma0, ]\n",
    "    for i in range(1, T+1):\n",
    "        e = gauss(0, 1)\n",
    "        sigma_next = np.sqrt(a0 + a1*(e*sigma[-1])**2 + b1*sigma[-1]**2)\n",
    "        sigma.append(sigma_next)\n",
    "    return np.array(sigma[1:])\n",
    "\n",
    "GARCH(a0, a1, b1, sigma0, 100)\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€ä¸ªGARCH\n",
    "sigma = GARCH(a0, a1, b1, sigma0, 2000)[200:]\n",
    "\n",
    "# å›¾å½¢è¾“å‡ºæ¨¡æ‹Ÿæ³¢åŠ¨ç‡\n",
    "plt.figure(figsize=(16,4))\n",
    "fig = plt.plot(sigma, linewidth=1)\n",
    "plt.title('Simulated Volatility')\n",
    "plt.show()\n",
    "\n",
    "#ç”¨archåŒ…æ‹ŸåˆGARCH(1, 1)æ¨¡å‹\n",
    "from arch.univariate import arch_model\n",
    "\n",
    "# å°†æ”¶ç›Šç‡æ•°æ®åˆ†åˆ—ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¸¤éƒ¨åˆ†\n",
    "train = hs300_return[:'2017'].dropna()\n",
    "test = hs300_return['2017':].dropna()\n",
    "train.tail()\n",
    "\n",
    "# æ„é€ å¹¶æ‹Ÿåˆä¸€ä¸ªGARCH(1,1)æ¨¡å‹\n",
    "garch = arch_model(train*100, mean='zero', p=1, o=0, q=1)  # æ”¶ç›Šä¹˜ä»¥100ï¼Œé¿å…æ•°å­—å¤ªå°é€ æˆçš„æ¨¡å‹ä¸ç¨³å®š\n",
    "res = garch.fit(disp='off')                              \n",
    "print(res.summary())\n",
    "\n",
    "# ä»æ‹Ÿåˆç»“æœä¸­æå–æ¡ä»¶æ³¢åŠ¨ç‡\n",
    "con_vol = res.conditional_volatility\n",
    "\n",
    "# æ¯”è¾ƒæ¡ä»¶æ³¢åŠ¨ç‡å’ŒçœŸå®æ”¶ç›Šç‡æ•°æ®çš„å›¾åƒ\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "fig.set_size_inches(16,8)\n",
    "ax1.plot(con_vol['2014':])\n",
    "ax1.set_title('Conditional Volatility')\n",
    "ax2.plot(train['2014':], lw=1)\n",
    "ax2.set_title('Hs300 Daily Return')\n",
    "plt.show()\n",
    "\n",
    "# ä½¿ç”¨æ‹Ÿåˆçš„æ¨¡å‹è¿›è¡Œæ ·æœ¬å†…é¢„æµ‹\n",
    "forecasts = res.forecast(horizon=1, start=train.index[0])                 # horizonæ˜¯é¢„æµ‹çš„æ­¥æ•°\n",
    "train_df = pd.concat([forecasts.mean, forecasts.variance**0.5], axis=1)   # æå–æ¨¡å‹é¢„æµ‹çš„å‡å€¼å’Œæ–¹å·®\n",
    "train_df.columns =['mu', 'sigma']\n",
    "train_df.tail()\n",
    "\n",
    "# æ ¹æ®é¢„æµ‹çš„å‡å€¼å’Œæ–¹å·®è®¡ç®—é¢„æµ‹æ”¶ç›Šç‡ ğ‘Ÿğ‘¡=ğœ‡ğ‘¡+ğœğ‘¡ğ‘§ğ‘¡\n",
    "train_df['z'] = np.random.normal(0, 1, len(data))\n",
    "train_df['r_hat'] = (data.mu + data.sigma*data.z)/100     # éœ€è¦é™¤ä»¥100ï¼Œ å› ä¸ºæ¨¡å‹æ‹Ÿåˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ˜¯daily_return*100\n",
    "train_df.head()\n",
    "\n",
    "# æ ·æœ¬å†…é¢„æµ‹å€¼å’ŒçœŸå®å€¼æ¯”è¾ƒ\n",
    "train_df.r_hat.plot(figsize=(16,5), c='b', alpha=0.7, lw=1, legend=True)\n",
    "hs300_return[:'2017'].plot(alpha=0.7, c='y', legend=True)\n",
    "plt.show()\n",
    "\n",
    "#ä½¿ç”¨æ‹Ÿåˆçš„æ¨¡å‹é¢„æµ‹æ³¢åŠ¨ç‡\n",
    "# åœ¨æµ‹è¯•æ•°æ®ä¸Šä½¿ç”¨æ‹Ÿåˆå¥½çš„æ¨¡å‹\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df['volatility_pre'] = (test_df.hs300_return*100).rolling(60).std().shift(1)   # å‰ä¸€å¤©çš„æ»šåŠ¨æ³¢åŠ¨ç‡\n",
    "test_df.dropna(inplace=True)\n",
    "test_df.head()\n",
    "\n",
    "# æ‹Ÿåˆæ¨¡å‹å¾—åˆ°çš„å‚æ•°\n",
    "res.params\n",
    "\n",
    "# ä»æ‹Ÿåˆæ¨¡å‹ä¸­æå–å‚æ•°\n",
    "omega, alpha, beta = res.params\n",
    "f'muï¼š0, omega: {omega:.4f}, alpha: {alpha:4f}, beta: {beta:.4f}'\n",
    "\n",
    "# ä½¿ç”¨æ‹Ÿåˆçš„å‚æ•°å’Œä¹‹å‰å®šä¹‰çš„æ¨¡æ‹Ÿå‡½æ•°é¢„æµ‹ä¸‹ä¸€ä¸ªæ³¢åŠ¨ç‡\n",
    "GARCH(omega, alpha, beta, 0.54, 1)\n",
    "\n",
    "# æ ¹æ®å‰ä¸€å¤©çš„æ»šåŠ¨æ³¢åŠ¨ç‡è®¡ç®—ä¸‹ä¸€å¤©çš„é¢„æµ‹æ³¢åŠ¨ç‡\n",
    "test_df['volatility_est'] = test_df.volatility_pre.map(lambda v: GARCH(omega, alpha, beta, v, 1)[-1])\n",
    "# æ ¹æ®é¢„æµ‹æ³¢åŠ¨ç‡è®¡ç®—é¢„æµ‹æ”¶ç›Šç‡\n",
    "test_df['return_est'] = (mu + test_df.volatility_est * np.random.normal(0,1, len(test_df)))/100\n",
    "test_df.head()\n",
    "\n",
    "# æ¯”è¾ƒæ ·æœ¬å¤–é¢„æµ‹ç»“æœä¸çœŸå®çš„æ”¶ç›Šç‡\n",
    "hs300_return['2013':].plot(figsize=(16,6), c='b', alpha=0.7, lw=1, legend=True)\n",
    "train_df['2013':].r_hat.plot(c='y', alpha=0.8, lw=1, legend=True)\n",
    "test_df['2018'].return_est.plot(c='aqua', alpha=0.5, legend=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
